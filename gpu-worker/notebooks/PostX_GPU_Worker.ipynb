{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# üöÄ PostX GPU Worker\n",
        "\n",
        "GPU Worker ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏†‡∏≤‡∏û‡πÅ‡∏•‡∏∞‡∏ß‡∏µ‡∏î‡∏µ‡πÇ‡∏≠‡∏î‡πâ‡∏ß‡∏¢ AI\n",
        "\n",
        "## ‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô\n",
        "1. ‡πÄ‡∏õ‡∏¥‡∏î GPU Runtime: Runtime > Change runtime type > GPU\n",
        "2. ‡∏£‡∏±‡∏ô‡∏ó‡∏∏‡∏Å Cell ‡∏ï‡∏≤‡∏°‡∏•‡∏≥‡∏î‡∏±‡∏ö\n",
        "3. ‡∏Ñ‡∏±‡∏î‡∏•‡∏≠‡∏Å ngrok URL ‡πÑ‡∏õ‡πÉ‡∏™‡πà‡πÉ‡∏ô‡πÇ‡∏õ‡∏£‡πÅ‡∏Å‡∏£‡∏° PostXAgent\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "header"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1Ô∏è‚É£ ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö GPU"
      ],
      "metadata": {
        "id": "check_gpu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gpu_check"
      },
      "outputs": [],
      "source": [
        "# ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö GPU\n",
        "!nvidia-smi\n",
        "\n",
        "import torch\n",
        "print(f\"\\n‚úÖ PyTorch version: {torch.__version__}\")\n",
        "print(f\"‚úÖ CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"‚úÖ GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"‚úÖ VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
        "else:\n",
        "    print(\"‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö GPU! ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡πÄ‡∏õ‡∏¥‡∏î GPU Runtime\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2Ô∏è‚É£ ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á Dependencies"
      ],
      "metadata": {
        "id": "install_deps"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á packages ‡∏ó‡∏µ‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô\n",
        "!pip install -q diffusers transformers accelerate safetensors\n",
        "!pip install -q fastapi uvicorn websockets pyngrok\n",
        "!pip install -q huggingface-hub pillow pynvml psutil\n",
        "\n",
        "print(\"‚úÖ ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô!\")"
      ],
      "metadata": {
        "id": "install"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3Ô∏è‚É£ ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ ngrok Token\n",
        "\n",
        "‡∏™‡∏°‡∏±‡∏Ñ‡∏£‡∏ö‡∏±‡∏ç‡∏ä‡∏µ ngrok ‡∏ü‡∏£‡∏µ‡∏ó‡∏µ‡πà: https://ngrok.com/\n",
        "\n",
        "‡πÅ‡∏•‡πâ‡∏ß‡∏Ñ‡∏±‡∏î‡∏•‡∏≠‡∏Å Auth Token ‡∏°‡∏≤‡πÉ‡∏™‡πà‡∏î‡πâ‡∏≤‡∏ô‡∏•‡πà‡∏≤‡∏á"
      ],
      "metadata": {
        "id": "ngrok_setup"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ‡πÉ‡∏™‡πà ngrok token ‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì‡∏ó‡∏µ‡πà‡∏ô‡∏µ‡πà\n",
        "NGROK_TOKEN = \"your_ngrok_token_here\"  # @param {type:\"string\"}\n",
        "\n",
        "# ‡∏ï‡∏±‡πâ‡∏á‡∏ä‡∏∑‡πà‡∏≠ Worker\n",
        "WORKER_NAME = \"Colab-GPU-Worker\"  # @param {type:\"string\"}\n",
        "\n",
        "# Master Server URL (‡∏ñ‡πâ‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠‡∏Å‡∏±‡∏ö PostXAgent)\n",
        "MASTER_URL = \"\"  # @param {type:\"string\"}\n",
        "\n",
        "# ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ ngrok\n",
        "if NGROK_TOKEN != \"your_ngrok_token_here\":\n",
        "    !ngrok authtoken {NGROK_TOKEN}\n",
        "    print(\"‚úÖ ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ ngrok token ‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡πÉ‡∏™‡πà ngrok token ‡∏Å‡πà‡∏≠‡∏ô\")"
      ],
      "metadata": {
        "id": "ngrok_config"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4Ô∏è‚É£ ‡∏™‡∏£‡πâ‡∏≤‡∏á GPU Worker Code"
      ],
      "metadata": {
        "id": "create_worker"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile gpu_worker.py\n",
        "\"\"\"\n",
        "PostX GPU Worker for Google Colab\n",
        "=================================\n",
        "\"\"\"\n",
        "import asyncio\n",
        "import logging\n",
        "import torch\n",
        "import io\n",
        "import base64\n",
        "import time\n",
        "import uuid\n",
        "from typing import Optional, List, Dict, Any\n",
        "from dataclasses import dataclass\n",
        "from PIL import Image\n",
        "\n",
        "from fastapi import FastAPI, HTTPException\n",
        "from fastapi.middleware.cors import CORSMiddleware\n",
        "from pydantic import BaseModel\n",
        "\n",
        "# Logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# ============================================================================\n",
        "# Models\n",
        "# ============================================================================\n",
        "\n",
        "@dataclass\n",
        "class GPUInfo:\n",
        "    id: int\n",
        "    name: str\n",
        "    memory_total: float\n",
        "    memory_used: float\n",
        "    memory_free: float\n",
        "    utilization: float = 0\n",
        "    temperature: float = 0\n",
        "\n",
        "\n",
        "class ImageRequest(BaseModel):\n",
        "    prompt: str\n",
        "    negative_prompt: str = \"\"\n",
        "    width: int = 1024\n",
        "    height: int = 1024\n",
        "    steps: int = 30\n",
        "    guidance_scale: float = 7.5\n",
        "    seed: int = -1\n",
        "    batch_size: int = 1\n",
        "    model_id: str = \"stabilityai/stable-diffusion-xl-base-1.0\"\n",
        "\n",
        "\n",
        "class TaskResponse(BaseModel):\n",
        "    task_id: str\n",
        "    status: str\n",
        "    message: str\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# GPU Monitor\n",
        "# ============================================================================\n",
        "\n",
        "def get_gpu_info() -> List[GPUInfo]:\n",
        "    \"\"\"Get GPU information\"\"\"\n",
        "    gpus = []\n",
        "    if torch.cuda.is_available():\n",
        "        for i in range(torch.cuda.device_count()):\n",
        "            props = torch.cuda.get_device_properties(i)\n",
        "            total = props.total_memory / 1024**2\n",
        "            used = torch.cuda.memory_allocated(i) / 1024**2\n",
        "            free = total - used\n",
        "            gpus.append(GPUInfo(\n",
        "                id=i,\n",
        "                name=props.name,\n",
        "                memory_total=total,\n",
        "                memory_used=used,\n",
        "                memory_free=free,\n",
        "            ))\n",
        "    return gpus\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# Image Generator\n",
        "# ============================================================================\n",
        "\n",
        "class ImageGenerator:\n",
        "    def __init__(self):\n",
        "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        self.dtype = torch.float16 if self.device == \"cuda\" else torch.float32\n",
        "        self.pipeline = None\n",
        "        self.current_model = None\n",
        "\n",
        "    def load_model(self, model_id: str):\n",
        "        if self.current_model == model_id:\n",
        "            return True\n",
        "\n",
        "        self.unload_model()\n",
        "        logger.info(f\"Loading model: {model_id}\")\n",
        "\n",
        "        try:\n",
        "            from diffusers import StableDiffusionXLPipeline\n",
        "\n",
        "            self.pipeline = StableDiffusionXLPipeline.from_pretrained(\n",
        "                model_id,\n",
        "                torch_dtype=self.dtype,\n",
        "                use_safetensors=True,\n",
        "                variant=\"fp16\" if self.dtype == torch.float16 else None,\n",
        "            )\n",
        "            self.pipeline = self.pipeline.to(self.device)\n",
        "            self.pipeline.enable_attention_slicing()\n",
        "            self.current_model = model_id\n",
        "            logger.info(f\"Model loaded: {model_id}\")\n",
        "            return True\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Failed to load model: {e}\")\n",
        "            return False\n",
        "\n",
        "    def unload_model(self):\n",
        "        if self.pipeline:\n",
        "            del self.pipeline\n",
        "            self.pipeline = None\n",
        "            self.current_model = None\n",
        "            if torch.cuda.is_available():\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "    def generate(self, request: ImageRequest) -> Dict[str, Any]:\n",
        "        if not self.load_model(request.model_id):\n",
        "            raise Exception(\"Failed to load model\")\n",
        "\n",
        "        seed = request.seed if request.seed >= 0 else torch.randint(0, 2**32-1, (1,)).item()\n",
        "        generator = torch.Generator(device=self.device).manual_seed(seed)\n",
        "\n",
        "        start_time = time.time()\n",
        "\n",
        "        output = self.pipeline(\n",
        "            prompt=request.prompt,\n",
        "            negative_prompt=request.negative_prompt or None,\n",
        "            width=request.width,\n",
        "            height=request.height,\n",
        "            num_inference_steps=request.steps,\n",
        "            guidance_scale=request.guidance_scale,\n",
        "            num_images_per_prompt=request.batch_size,\n",
        "            generator=generator,\n",
        "        )\n",
        "\n",
        "        generation_time = time.time() - start_time\n",
        "\n",
        "        # Convert to base64\n",
        "        images_b64 = []\n",
        "        for img in output.images:\n",
        "            buffer = io.BytesIO()\n",
        "            img.save(buffer, format=\"PNG\")\n",
        "            buffer.seek(0)\n",
        "            images_b64.append(base64.b64encode(buffer.read()).decode())\n",
        "\n",
        "        return {\n",
        "            \"images\": images_b64,\n",
        "            \"seed\": seed,\n",
        "            \"generation_time\": generation_time,\n",
        "            \"model_id\": request.model_id,\n",
        "        }\n",
        "\n",
        "\n",
        "# Global generator\n",
        "generator = ImageGenerator()\n",
        "\n",
        "# ============================================================================\n",
        "# FastAPI App\n",
        "# ============================================================================\n",
        "\n",
        "app = FastAPI(title=\"PostX GPU Worker\", version=\"1.0.0\")\n",
        "\n",
        "app.add_middleware(\n",
        "    CORSMiddleware,\n",
        "    allow_origins=[\"*\"],\n",
        "    allow_credentials=True,\n",
        "    allow_methods=[\"*\"],\n",
        "    allow_headers=[\"*\"],\n",
        ")\n",
        "\n",
        "# State\n",
        "task_results = {}\n",
        "\n",
        "\n",
        "@app.get(\"/\")\n",
        "async def root():\n",
        "    return {\"service\": \"PostX GPU Worker\", \"status\": \"running\"}\n",
        "\n",
        "\n",
        "@app.get(\"/health\")\n",
        "async def health():\n",
        "    return {\"status\": \"healthy\"}\n",
        "\n",
        "\n",
        "@app.get(\"/status\")\n",
        "async def get_status():\n",
        "    gpus = get_gpu_info()\n",
        "    return {\n",
        "        \"status\": \"online\",\n",
        "        \"gpu_count\": len(gpus),\n",
        "        \"gpus\": [{\n",
        "            \"id\": g.id,\n",
        "            \"name\": g.name,\n",
        "            \"memory_total_gb\": g.memory_total / 1024,\n",
        "            \"memory_free_gb\": g.memory_free / 1024,\n",
        "        } for g in gpus],\n",
        "        \"total_vram_gb\": sum(g.memory_total for g in gpus) / 1024,\n",
        "        \"free_vram_gb\": sum(g.memory_free for g in gpus) / 1024,\n",
        "        \"current_model\": generator.current_model,\n",
        "    }\n",
        "\n",
        "\n",
        "@app.post(\"/generate/image\")\n",
        "async def generate_image(request: ImageRequest):\n",
        "    task_id = str(uuid.uuid4())\n",
        "    \n",
        "    try:\n",
        "        result = await asyncio.to_thread(generator.generate, request)\n",
        "        task_results[task_id] = {\n",
        "            \"status\": \"completed\",\n",
        "            \"result\": result\n",
        "        }\n",
        "        return {\n",
        "            \"task_id\": task_id,\n",
        "            \"status\": \"completed\",\n",
        "            \"result\": result\n",
        "        }\n",
        "    except Exception as e:\n",
        "        task_results[task_id] = {\n",
        "            \"status\": \"failed\",\n",
        "            \"error\": str(e)\n",
        "        }\n",
        "        raise HTTPException(status_code=500, detail=str(e))\n",
        "\n",
        "\n",
        "@app.get(\"/task/{task_id}\")\n",
        "async def get_task(task_id: str):\n",
        "    if task_id not in task_results:\n",
        "        raise HTTPException(status_code=404, detail=\"Task not found\")\n",
        "    return task_results[task_id]\n",
        "\n",
        "\n",
        "@app.post(\"/model/load\")\n",
        "async def load_model(model_id: str):\n",
        "    success = await asyncio.to_thread(generator.load_model, model_id)\n",
        "    if success:\n",
        "        return {\"message\": f\"Model {model_id} loaded\"}\n",
        "    raise HTTPException(status_code=500, detail=\"Failed to load model\")\n",
        "\n",
        "\n",
        "@app.post(\"/model/unload\")\n",
        "async def unload_model():\n",
        "    generator.unload_model()\n",
        "    return {\"message\": \"Model unloaded\"}\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    import uvicorn\n",
        "    uvicorn.run(app, host=\"0.0.0.0\", port=8080)"
      ],
      "metadata": {
        "id": "worker_code"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5Ô∏è‚É£ ‡πÄ‡∏£‡∏¥‡πà‡∏° GPU Worker"
      ],
      "metadata": {
        "id": "start_worker"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "\n",
        "from pyngrok import ngrok\n",
        "import threading\n",
        "import uvicorn\n",
        "\n",
        "# Import worker\n",
        "from gpu_worker import app\n",
        "\n",
        "# Start uvicorn in background\n",
        "def run_server():\n",
        "    uvicorn.run(app, host=\"0.0.0.0\", port=8080, log_level=\"info\")\n",
        "\n",
        "thread = threading.Thread(target=run_server, daemon=True)\n",
        "thread.start()\n",
        "\n",
        "# Wait for server to start\n",
        "import time\n",
        "time.sleep(3)\n",
        "\n",
        "# Start ngrok tunnel\n",
        "public_url = ngrok.connect(8080)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üöÄ PostX GPU Worker ‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô!\")\n",
        "print(\"=\"*60)\n",
        "print(f\"\\nüì° Public URL: {public_url}\")\n",
        "print(f\"\\nüìã ‡∏Ñ‡∏±‡∏î‡∏•‡∏≠‡∏Å URL ‡∏ô‡∏µ‡πâ‡πÑ‡∏õ‡πÉ‡∏™‡πà‡πÉ‡∏ô‡πÇ‡∏õ‡∏£‡πÅ‡∏Å‡∏£‡∏° PostXAgent\")\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "\n",
        "# Test endpoint\n",
        "import requests\n",
        "try:\n",
        "    response = requests.get(f\"{public_url}/status\")\n",
        "    status = response.json()\n",
        "    print(f\"\\n‚úÖ GPU: {status['gpus'][0]['name'] if status['gpus'] else 'N/A'}\")\n",
        "    print(f\"‚úÖ VRAM: {status['total_vram_gb']:.1f} GB\")\n",
        "except:\n",
        "    print(\"‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠‡πÑ‡∏î‡πâ ‡∏£‡∏≠‡∏™‡∏±‡∏Å‡∏Ñ‡∏£‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß‡∏•‡∏≠‡∏á‡πÉ‡∏´‡∏°‡πà\")"
      ],
      "metadata": {
        "id": "start"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6Ô∏è‚É£ ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏†‡∏≤‡∏û"
      ],
      "metadata": {
        "id": "test_section"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import base64\n",
        "from PIL import Image\n",
        "import io\n",
        "from IPython.display import display\n",
        "\n",
        "# ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏†‡∏≤‡∏û\n",
        "prompt = \"A beautiful sunset over the ocean, photorealistic, 8k\"  # @param {type:\"string\"}\n",
        "\n",
        "print(\"üé® ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏†‡∏≤‡∏û... (‡∏≠‡∏≤‡∏à‡πÉ‡∏ä‡πâ‡πÄ‡∏ß‡∏•‡∏≤ 30-60 ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ)\")\n",
        "\n",
        "response = requests.post(\n",
        "    f\"{public_url}/generate/image\",\n",
        "    json={\n",
        "        \"prompt\": prompt,\n",
        "        \"width\": 1024,\n",
        "        \"height\": 1024,\n",
        "        \"steps\": 30,\n",
        "    }\n",
        ")\n",
        "\n",
        "if response.status_code == 200:\n",
        "    result = response.json()\n",
        "    print(f\"\\n‚úÖ ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏†‡∏≤‡∏û‡πÄ‡∏™‡∏£‡πá‡∏à‡πÉ‡∏ô {result['result']['generation_time']:.1f} ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ\")\n",
        "    print(f\"Seed: {result['result']['seed']}\")\n",
        "    \n",
        "    # ‡πÅ‡∏™‡∏î‡∏á‡∏†‡∏≤‡∏û\n",
        "    for img_b64 in result['result']['images']:\n",
        "        img_data = base64.b64decode(img_b64)\n",
        "        img = Image.open(io.BytesIO(img_data))\n",
        "        display(img)\n",
        "else:\n",
        "    print(f\"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î: {response.text}\")"
      ],
      "metadata": {
        "id": "test_generate"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üìå ‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏´‡∏ï‡∏∏\n",
        "\n",
        "- Colab ‡∏à‡∏∞‡∏ï‡∏±‡∏î‡∏Å‡∏≤‡∏£‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô ~30 ‡∏ô‡∏≤‡∏ó‡∏µ\n",
        "- Colab Pro ‡∏à‡∏∞‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡πÑ‡∏î‡πâ‡∏ô‡∏≤‡∏ô‡∏Å‡∏ß‡πà‡∏≤ ‡πÅ‡∏•‡∏∞ GPU ‡πÅ‡∏£‡∏á‡∏Å‡∏ß‡πà‡∏≤\n",
        "- ‡∏´‡∏≤‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏ï‡πà‡∏≠‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡πÉ‡∏´‡πâ‡πÄ‡∏ä‡πà‡∏≤ GPU ‡∏à‡∏≤‡∏Å Runpod, Vast.ai ‡∏´‡∏£‡∏∑‡∏≠ Lambda Labs\n",
        "\n",
        "---\n",
        "\n",
        "Made with ‚ù§Ô∏è by PostXAgent Team"
      ],
      "metadata": {
        "id": "notes"
      }
    }
  ]
}
